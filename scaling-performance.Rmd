# Performance {#performance}

```{r, include = FALSE}
source("common.R")
options(tibble.print_min = 6, tibble.print_max = 6)
```

Shiny can support thousands or tens of thousands of users, if developed correctly.

There are three main ways to make your app faster:

-   Make it faster for a single user.
    Not necessarily easy, but also not directly related to shiny.
    If you're concerned about single user performance (i.e.Â it's just you using the app), it's very unlikely that Shiny is the bottleneck.
    Instead, improving the performance of your app is really about improving the performance of your R code.
    Many of the same techniques apply; in particular you'll still need to profile the performance of your app.
    Some advice in [\<https://adv-r.hadley.nz/perf-measure.html\>](https://adv-r.hadley.nz/perf-measure.html){.uri} and [\<https://adv-r.hadley.nz/perf-improve.html\>](https://adv-r.hadley.nz/perf-improve.html){.uri}.

-   Make it faster for multiple users on the same server.

-   Make it faster by running more servers.
    Shiny is horizontally scalable, which means that you can always make it faster by providing more servers.
    Shiny and R scale linearly per processor --- if you need to support more users you can just use more processors.

Here we'll focus mostly on the unique challenge of speeding up Shiny apps, mixed in with some practical strategies that we've seen to be useful for many.
Focus is on increasing the performance for multiple users.

Performance issues typically arise in high-stress situations.
So good to have a process to follow.
Benchmark (is it too slow?) -\> analyse (why is it slow?) -\> recommend (how can I make it faster?) -\> optimise (does it work) -\> repeat.

```{r setup}
library(shiny)
```

Particularly thanks go to my RStudio colleagues Joe Cheng, Sean Lopp, and Alan Dipert, whose RStudio::conf() talks were particularly helpful when writing this chapter.

-   <https://rstudio.com/resources/rstudioconf-2019/shiny-in-production-principles-practices-and-tools/>

-   <https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/>

-   <https://rstudio.com/resources/rstudioconf-2018/make-shiny-fast-by-doing-as-little-work-as-possible/>

## Dining at the Shiny restaurant

To help improve your mental model of app serving, it's useful to think of a Shiny app like a restaurant.
Each customer (user) comes in and makes an order which is prepared and served by a chef (the R process).
One chef can server multiple customers, but as you get more customers, you're likely to want to hire more chefs.
But at some point there's so many that they get in each others' way --- there's still only one oven, one grill, one refrigerator, etc.
So at some point, you're going to need to add a new restaurant (server).

Just like you can serve a customer meal made in multiple restaurants with a new restaurant, there's no way to serve one user from multiple servers, so this also needs to be coupled with a load balancer which automatically sends customers to the emptiest restaurant.
We won't talk more about scaling out in this chapter (because while the details are straightforward, they depend entirely on your deployment infrastructure), but it's good to know this option exists, and allows you app to scale to any number of users.

[\<https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/\>](https://rstudio.com/resources/rstudioconf-2018/scaling-shiny/){.uri}

(R is single threaded which means the chef is very focussed --- they can only cook one thing at a time. This is fine if they're doing something quick, but if you ask them to make a stew, all the other customers have to wait for a couple of hours until they're done. While you can work around this problem by adding more single track chefs, a better way is to teach the chef how to work on multiple dishes simultaneously. This is the idea of promises: <https://rstudio.github.io/promises/>)

## Benchmark

To begin, it's important to figure out how many customers each chef can serve (how many users per R process) and how many chefs can fit in a kitchen (how many processes per server).

Not is it fast: you can always make it faster.
Most important question is: Is it fast **enough**?

Start with a plan:

-   How many people?
    How many people at the same time?

-   What's the computing budget?
    i.e.Â how many cores, how many machines?

-   What's the time budget?

Benchmark is a model of what user does.
What exactly do you care about?
Then use that to generate some metrics that actually recorded.
Then figure what "good" means.

Question becomes whether it's cheaper to pay for more computing, or to pay for your time to fit more users on one computer.
Typically it's going to be combination, because there are often many big easy wins because you've worked on the app on your own computer as a solo user.

### Load testing

shinyloadtest + shinycannon

-   Run or deploy your app.

-   Record an archetypal app session: `shinyloadtest::record_session()`.
    Need to make as realistic as possible

-   Replay with multiple users using shinycannon on the terminal.

-   Then analyse: `shinyloadtest::load_runs()` `shinyloadtest::report()`.

[\<https://rstudio.github.io/shinyloadtest/\>](https://rstudio.github.io/shinyloadtest/){.uri}

Each row of plot is a simulated work.
x axis is time.
red = time to load.
Blue = reactive computation.
Gray = user thinking.

## Measure

If you want a chef to serve more customers, you need to do some time and motion studies to figure out what's slowing them down.
The equivalent in R is profiling, which basically regularly inspects the running code and records the call stack at each instant.

Note that it only records when the R is active; not when it's waiting (e.g. in `Sys.sleep()` or when downloading data over http), or when C code is being called.
This can be misleading, but does serve to concentrate your attention on what you can actually control within R.

What is call stack.
Section \@ref(reading-tracebacks).
Call stacks grow and shrink over time.

```{r, eval = FALSE}
library(profvis)
profvis(runApp())
# perform the operation that's slow
# close the app
# look at the visualisation
```

Call stack diagram --- show code, then draw tree, then collapse into rectangles, then make width proportional to time.

Goal is to find the one slowest thing, because that has the highest payoff.
Once you've found it, brainstormed possible improvements and then tried them out, you look for the next slower thing.

Once you've isolated a slow part, if it's not already in a function, I highly recommend pulling it out as in Chapter \@ref(scaling-functions).
That will make it much easier to optimise.
Also recommend testing it, because it at least in my experience the easiest way to make code faster is to make it incorrect ðŸ˜†.

## Do less work

Most techniques are general --- follow advice in Advanced R. But there's some particular techniques unique to Shiny because of the multiple users.
Often you can save time by sharing work across users.
Don't repeat yourself.

### Data import

First, make sure that any common data is loaded outside of the server function, in the body of the `app.R`.
That ensures that the data is once per process, rather than once per user, which saves both time and memory.

Next, check that you're using the most efficient way to load your data:

-   If you have a flat file, try `data.table::fread()` or `vroom::vroom()` instead of `read.csv()` or `read.table()`.

-   If you have a data frame, saving with `arrow::write_feather()` and reading, try `arrow::read_feather()`.
    (<https://ursalabs.org/blog/2020-feather-v2/>)

-   Complex non-data frame, try `qs::qread()`/`qs::qsave()` instead of `readRDS()`/`saveRDS()`.

If that's still too slow, and each user only tends to use a small part of the full dataset, consider loading the data in a database.
Then you can easily retrieve only the data that the user specifically asks for.

### Data processing

After loading data from disk, it's common to do some basic cleaning and aggregation.
If this is expensive, you should consider using cron job (or scheduled RMarkdown reports) or similar to save the precomputed results.
To continue the restaurant analogy --- this is like hiring a prep chef who comes in at 3am (when there are no customers) and does a bunch of work so that that chefs can be as efficient as possible.

### Share work across users

We discussed a specific type of caching for graphics in Section \@ref(cached-plots).
Shiny 1.6.0 introduces a general tool that works with any reactive: `withCache()`.
By default, reactives are already cached, but they only cache the previous value.
`withCache()` allows you to cache more values and to share those values across users.

To use the cache effectively, you'll need to have identified that a specific reactive is a bottleneck and done some thinking to make sure that the reactive is used multiple times or by multiple users.
(Also note that the impact of caching on your load tests is likely to be an over estimated because every simulated user does exactly the same thing, making it a perfect use case for caching).
Then:

`withCache()` is easy to use.
Just pipe the reactive into `withCache()`:

```{r, eval = FALSE}
r <- reactive(slow_function(input$x, input$y)) %>% 
  withCache(input$x, input$y)
```

The extra arguments to `withCache()` are the cache keys --- these are the values used to determine if a computation has occurred before and hence can be retrieved from the cache.

`withCache()` is usually paired with `withEvent()` because if a computation takes long enough that it's worth caching it, it's likely that you'll want to user to manually trigger with an action button or similar.

```{r, eval = FALSE}
r <- reactive() %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```

Like `renderCachedPlot()`, `withCache()` has a scope setting.
It defaults to `app` so that you get an in memory cache shared across all users of the app.
But you can `scope = "session"` so that each user session gets its own cache, or to `cachem::disk_cache()` to share across users, processes, and app restarts.
The more aggressively you cache, you more care you'll need to take to manually clear the cache when you change behaviour (e.g. the computation in a reactive) that's not captured by the cache key.

## Manage user expectations

As well as making your app faster, you can also make it seem faster.

<https://www.nngroup.com/articles/progress-indicators/>

Require confirmation before known slow interaction.
Show a Progress bar.
Techniques of Chapter \@ref(action-feedback)

```{r, eval = FALSE}
r <- reactive({
  id <- showNotification("Reading data...", duration = NULL, closeButton = FALSE)
  on.exit(removeNotification(id), add = TRUE)
  
  read.csv(input$path)
}) %>% 
  withCache(input$x, input$y) %>% 
  withEvent(input$go)
```
