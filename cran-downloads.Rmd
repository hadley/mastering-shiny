# CRAN download logs {#cran-downloads}

```{r include=FALSE}
source("common.R")
```

<!--
Case study ideas:
https://openintro.shinyapps.io/CLT_mean/
https://gallery.shinyapps.io/correlation_game/
-->

## Introduction

In the last three chapters, we've introduced you to a bunch of new concepts. To help them sink in, we'll now walk through how you might take an existing R script and turn it into a shiny app. This will help you start to see how the pieces fit together, and give you some useful tools to help analyse your own code.

In this chapter, we're going to supplement Shiny with the tidyverse and lubridate packages.

```{r setup, message = FALSE}
library(tidyverse)
library(lubridate)
library(shiny)
```

## Motivation

Our goal is to analyse some data about R package downloads from <http://cran-logs.rstudio.com>. This dataset records every time a package is installed from the RStudio CRAN mirror.

The complete data set is very large so it is broken up into one csv file for each day. There are many things we might do with this data, but here we're going to look at the most downloaded packages, as well as exploring how the number of downloads tails of for less popular packages.

### Download

We'll start by downloading a single file. The files are quite large, so downloading takes a long time. This means it's worth writing a little code to only download it if needed. This makes the code both efficient and means that we can run it on a new computer if needed[^git].

[^git]: If you're using git, I'd recommend adding `cran-logs/` to your `.gitignore`. Since you can easily redownload them when needed there's no need to track separately, and git generally isn't well designed for large data files.

```{r}
dir.create("cran-logs", showWarnings = FALSE)
url <- "http://cran-logs.rstudio.com/2019/2019-02-20.csv.gz"
path <- "cran-logs/2019-02-20.csv.gz"

if (!file.exists(path)) {
  download.file(url, path, method = "libcurl", quiet = TRUE)
}

# Size in MB
file.size("cran-logs/2019-02-20.csv.gz") / (1024 ^ 2)
```

This is a useful pattern which we're going to rely on later, so we'll wrap it up into a function:

```{r}
read_cran_log <- function(date) {
  date <- ymd(date)
  url <- sprintf(
    "http://cran-logs.rstudio.com/%s/%s.csv.gz", 
    year(date),
    date
  )
  path <- paste0("cran-logs/", date, ".csv.gz")

  if (!file.exists(path)) {
    download.file(url, path, method = "libcurl", quiet = TRUE)
  }
  
  vroom::vroom("cran-logs/2019-02-20.csv.gz", delim = ",")
}
```

### Read and inspect

Then we can read it in and look at it. Here I use [vroom](https://vroom.r-lib.org), which is generally a fast and convenient way to read in flat files:

```{r}
logs <- vroom::vroom("cran-logs/2019-02-20.csv.gz", delim = ",")
logs
```

There are 3,437,899 rows and 10 columns: 

* `date` and `time`, when the package was downloaded.

* download `size`, in bytes.

* `r_version`, the version of R.

* `r_arch`, the machine architecture (e.g. `"x86_64"`, 64-bit, or `"i386"`,
  32-bit).

* `r_os`, the operating system.

* `package`, the package being requested.

* `version`, the exact version of the package being requested.

* `country`, imputed from the IP address using geo location. (Geo-location
  is not always terribly accurate, should be pretty robust at the country
  level.)

* `ip_id`, an anonymized identifier for the IP address of the requestor

`r_version`, `r_arch`, `r_os` are only present for binary packages. Binary packages are what is usually installed on windows and mac computers. Binary packages are not available for linux computers, instead they download source packages and then compile locally.

### Aggregate

After downloading the data, the script will tally the number of rows (downloads) with each value in a specified column. For example, if we specify the `package` column, then we get the number of times each package was downloaded; if we specify the `r_os` column, we get the number of downloads from each operating system. 

```{r}
dimension <- "package"

# D. Count observations, grouping by the desired dimension
by_frequency <- logs %>% count(.data[[dimension]], sort = TRUE)
by_frequency
```

The implementation details here are not that important; just understand that we're taking the `download_data` data frame from above, with its 3.4 million rows, and turning it into counts of rows grouped by the dimension in question (`package`). You might not have seen the form `.data[[var]]` before: this is the recommended way to refer to a variable that is recorded as a string.  

### Visualise

Once the tallying is done, we'll plot this data in a couple of ways. We'll zoom in on the most popular packages and show exactly how many times they were downloaded:

```{r ggplots}
most_frequent <- by_frequency %>% head(20)

# F. Plot the top values
most_frequent %>% 
  ggplot(aes(fct_reorder(.data[[dimension]], n), n / 1000)) +
  geom_col() +
  coord_flip() +
  labs(
    x = dimension,
    y = "Downloads (000s)",
    title = paste0("CRAN download counts by ", dimension)
  )
```

We might also consider looking at the least frequent:

```{r}
by_frequency %>% tail(-20)
```

And we'll also visualize all the values to get a sense of the "long tail", i.e. how quickly the number of downloads decrease as we move down the list of packages.

```{r}
# G. Plot all of the counts, by rank ("long tail")
by_frequency %>% 
  ggplot(aes(seq_along(n), n)) +
  geom_line() +
  scale_y_log10() +
  labs(
    x = "Rank",
    title = paste0("CRAN download counts by ", dimension, " rank")
  )
```

## Making an app

To transform this code into a Shiny app, we need to answer a few questions:

* What are the inputs and outputs?
* What should the app look like?
* How are the inputs and outputs connected together?

### What are the inputs and outputs?

We've written this script with a few obvious knobs:

* `date`: the date to download and explore
* `dimension`: which variable we want to group the data by
* `top_count`: how many categories to show individually.

Next we need to figure out which input control we'd use for each:

* `date`: `dateIntput("date")`
* `dimension`: `selectInput("dimension", c("package", "r_version", "r_arch", "r_os", "country", "ip_id"))`
* `top_count`: `numericInput("top_count", value = 20, min = 1)`

What are the outputs from the script?

We have two plots.

We'll also show a table of the least popular.

### What should the app look like?

This is enough information to create the front-end of the app. We just need to think about how to layout the inputs and outputs. Often the easiest way to do this is to make a few quick sketches by hand. This allows you to easily explore the basic layout before starting to write code.

```{r}
variables <- c("package", "r_version", "r_arch", "r_os", "country")

ui <- fluidPage(
  fluidRow(
    column(4, dateInput("date", "Date", value = "2019-02-20")),
    column(4, selectInput("by", "Grouped by", choices = variables)),
    column(4, numericInput("n", "Top", value = 20, min = 0))
  ),
  fluidRow(
    column(4, plotOutput("top_n")),
    column(4, tableOutput("bottom")),
    column(4, plotOutput("rank"))
  )
)
```


### How are the inputs and outputs connected?

```{r}
server <- function(input, output, session) {
  logs <- reactive({
    note <- showNotification("Reading data", duration = NULL)
    on.exit(removeNotification(note))
    read_cran_log(input$date)
  })
  
  freq <- reactive({
    message("Counting")
    logs() %>% count(.data[[input$by]], sort = TRUE)
  })
  
  output$top_n <- renderPlot({
    print(head(freq(), input$n))
    
    freq() %>% 
      head(input$n) %>% 
      ggplot(aes(fct_reorder(.data[[input$by]], n), n / 1000)) +
      geom_col() +
      coord_flip() +
      labs(
        x = dimension,
        y = "Downloads (000s)",
        title = paste0("CRAN download counts by ", input$by)
      )
  })
  
  output$next_n <- renderTable({
    freq()[(input$n + 1):(input$n * 2), ]
  })
  
  output$rank <- renderPlot({
    freq() %>% 
      ggplot(aes(seq_along(n), n)) +
      geom_line() +
      coord_flip() +
      scale_y_log10() +
      labs(
        x = "Rank",
        title = paste0("CRAN download counts by ", input$by, " rank")
      )
  })
}
```

### Break down

You can already tell by the code comments above that I've divided the work into discrete steps. Incidentally, this is good practice anyway, as is breaking larger scripts into smaller functions (which I did not do in this case). Whatever form of subdivision you use, breaking down the problem into smaller subproblems makes our code easier to write, read, and debug.

I've summarized these steps in the table below.

|Step|Variable       |Description|
|----|---------------|-----------|
|A   |`date`, `dimension`, `top_count`|Configuration variables|
|B   |`url`          |Form URL for downloading|
|C   |`download_df`  |Download/parse data|
|D   |`by_frequency` |Count observations, grouping by the desired dimension|
|E   |`most_frequent`|Limit to the top `top_count`|
|F   |               |Plot the top values|
|G   |               |Plot all of the counts|

This is the breakdown I'm using, but there isn't one single correct arrangement of steps; you may consider steps B and C to really be a single step. These changes wouldn't cause any noticeable change in the readability of our code or the performance of our app.

We could also consider breaking `read_cran_log()` up into smaller pieces. That would make it possible for the downloading and reading to be cache separately. I don't think it would help for this app, but in general, if you find existing functions do too much work (so you can't efficiently capture intermediates) you might need to think about breaking into smaller pieces.

But just because there isn't a single right answer, does _not_ mean there are no wrong answers! We'll discuss some of the factors we want to consider in a section below.

